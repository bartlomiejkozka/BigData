{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "598c53b8-a653-419d-a239-2074b9a174de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DLT Wymagane biblioteki\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15db1f16-2791-4303-ba2e-ef72b1473b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import current_date, expr, col, lit, hash, regexp_extract, current_timestamp\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aec4594-14cd-43cb-a891-5085709270a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parametry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e5fa700-5957-4451-97db-842f9b4145d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "param_environment = spark.conf.get(\"param_environment\", \"dev\")\n",
    "param_source_name = spark.conf.get(\"param_source_name\", \"\")\n",
    "schema = spark.conf.get(\"silver_schema\")\n",
    "param_scd_type = spark.conf.get(\"param_scd_type\", \"scd1\")\n",
    "bronze_schema = spark.conf.get(\"bronze_schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5290afd7-5dc1-47f0-ad65-8f94704a7101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pobierasz z bronze \n",
    "\n",
    "- Definjuje tabele bronze \n",
    "- Pobieram dane ze schematu bronze.dbo przy użciu spark.readStream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4267513-863a-45b2-9461-26bb97749031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Jak działa funkcja `silver_table_scd1` w Delta Live Tables\n",
    "\n",
    "Funkcja dekorowana `@dlt.table` tworzy w potoku **Delta Live Tables** (DLT) tabelę warstwy *silver* o nazwie `silver.<schema>.<param_source_name>` i oznacza ją metadanymi:\n",
    "\n",
    "* `quality = \"silver\"` – poziom jakości danych  \n",
    "* `scd_type = \"scd1\"` – uproszczona implementacja Slowly Changing Dimension Type 1 \n",
    "\n",
    "### Logika w funkcji\n",
    "\n",
    "1. **Sprawdzenie parametru**  \n",
    "   Funkcja działa tylko, gdy zewnętrzny parametr `param_scd_type` ma wartość `\"scd1\"`. W innym przypadku zwraca pusty DataFrame, dzięki czemu pipeline się nie wywraca.\n",
    "\n",
    "2. **Odczyt danych z warstwy *bronze***  \n",
    "   ```python\n",
    "   df_bronze = dlt.read_stream(f\"bronze.{schema}.{param_source_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eb9ecb7-3214-4cfa-9d71-6f1c3ded59bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=f\"{schema}.{param_source_name}\",\n",
    "    comment=f\"Silver table for {param_source_name} with SCD Typ 1\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"scd_type\": \"scd1\"\n",
    "    }\n",
    ")\n",
    "def silver_table_scd1():\n",
    "    \"\"\"\n",
    "    SCD Typ 1\n",
    "    \"\"\"\n",
    "    if param_scd_type == \"scd1\":\n",
    "        # tabela źrodłowa\n",
    "        df_bronze = dlt.read_stream(f\"{bronze_schema}.{param_source_name}\")\n",
    "        \n",
    "        df_processed = (df_bronze\n",
    "            .dropDuplicates()\n",
    "            .withColumn(\"hash_value\", hash(*[col for col in df_bronze.columns if col not in ['_rescued_data']]))\n",
    "            .withColumn(\"rating_value\", regexp_extract('rating', r'(\\d+\\.?\\d*)', 1).cast(\"double\"))\n",
    "            .withColumn(\"item_weight\", regexp_extract('item_weight', r'(\\d+\\.?\\d*)', 1).cast(\"double\"))\n",
    "            .withColumn(\"inserted_at\", current_timestamp())\n",
    "            .withColumn(\"updated_at\", current_timestamp())\n",
    "            .withColumn(\"scd_is_current\", lit(True))\n",
    "            .withColumn(\"scd_start_date\", current_date())\n",
    "            .withColumn(\"scd_end_date\", lit('9999-12-31'))\n",
    "        )\n",
    "        \n",
    "        return df_processed\n",
    "    else:\n",
    "        return spark.createDataFrame([], schema=\"dummy STRING\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT_02_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}