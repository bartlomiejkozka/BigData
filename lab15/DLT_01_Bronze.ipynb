{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b85b4a-ca0f-4a5f-97ba-205b3d9312f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Podstawy procesu DLT\n",
    "\n",
    "Notatnik Delta Live Tables (DLT) przetważa pliki źrodłowe JSON i używa funkcjinalnośći autoloadera z poprzednich etapów kursu. \n",
    "Jest to piersza część medalioinu ładująca dane z warsty raw do bronze.  \n",
    "\n",
    "* Ten notatnik zasila pierwszą warstwę bonze zawiera dane ze źródła, nie przetwożone.\n",
    "\n",
    "\n",
    "Celem notanika jest:\n",
    "* Definicja Delta Live Tables\n",
    "* Ładowanie danych z Auto Loader\n",
    "* Uzycie parametrów DLT Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4757f5-7473-48bf-86ff-6b89fbb68bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Notatnik DLT\n",
    "\n",
    "Serverless nie uruchomi kodu DLT, musi być pelny runtime. \n",
    "W związku z tym trzeba storzyć pipeline który uruchomi kod \n",
    "\n",
    "## Parametry\n",
    "\n",
    "Podczas tworzenia pipeline dodajemy parametry w polu konfiguracje\n",
    "\n",
    "\n",
    "Te parametry należą do konfiguracji Sparka.\n",
    "\n",
    "In Python, we can access these values using **`spark.conf.get()`**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69cbe715-5cee-469d-9e7c-de997c2721b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "      <div style=\"font-size:18px\">\n",
       "        The Delta Live Tables (DLT) module is not supported on this cluster.\n",
       "        You should either <a href=\"?o=411645875212786#joblist/pipelines/create?initialSource=%2FShared%2FDLT%2FDLT_01_Bronze&redirectNotebookId=2229313379879411\">create a new pipeline</a> or use an existing pipeline to run DLT code.\n",
       "      </div>\n",
       "    </html>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "      <div style=\"font-size:18px\">\n",
       "        The Delta Live Tables (DLT) module is not supported on this cluster.\n",
       "        You should either <a href=\"?o=411645875212786#joblist/pipelines/create?initialSource=%2FShared%2FDLT%2FDLT_01_Bronze&redirectNotebookId=2229313379879411\">create a new pipeline</a> or use an existing pipeline to run DLT code.\n",
       "      </div>\n",
       "    </html>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2229313379879429>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m param_environment \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparam_environment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdev\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      4\u001B[0m param_source_name \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparam_source_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/autoreload/discoverability/hook.py:71\u001B[0m, in \u001B[0;36mAutoreloadDiscoverabilityHook._patched_import\u001B[0;34m(self, name, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint \u001B[38;5;129;01mand\u001B[39;00m (\n",
       "\u001B[1;32m     66\u001B[0m     (module \u001B[38;5;241m:=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules\u001B[38;5;241m.\u001B[39mget(absolute_name)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n",
       "\u001B[1;32m     67\u001B[0m     (fname \u001B[38;5;241m:=\u001B[39m get_allowed_file_name_or_none(module)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n",
       "\u001B[1;32m     68\u001B[0m     (mtime \u001B[38;5;241m:=\u001B[39m os\u001B[38;5;241m.\u001B[39mstat(fname)\u001B[38;5;241m.\u001B[39mst_mtime) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_mtime_by_modname\u001B[38;5;241m.\u001B[39mget(\n",
       "\u001B[1;32m     69\u001B[0m         absolute_name, \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint):\n",
       "\u001B[1;32m     70\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[0;32m---> 71\u001B[0m module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_builtins_import(name, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (fname \u001B[38;5;241m:=\u001B[39m fname \u001B[38;5;129;01mor\u001B[39;00m get_allowed_file_name_or_none(module)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m     73\u001B[0m     mtime \u001B[38;5;241m=\u001B[39m mtime \u001B[38;5;129;01mor\u001B[39;00m os\u001B[38;5;241m.\u001B[39mstat(fname)\u001B[38;5;241m.\u001B[39mst_mtime\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'dlt'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'dlt'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'dlt'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-2229313379879429>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n\u001B[1;32m      3\u001B[0m param_environment \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparam_environment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdev\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m param_source_name \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparam_source_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/autoreload/discoverability/hook.py:71\u001B[0m, in \u001B[0;36mAutoreloadDiscoverabilityHook._patched_import\u001B[0;34m(self, name, *args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m     66\u001B[0m     (module \u001B[38;5;241m:=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules\u001B[38;5;241m.\u001B[39mget(absolute_name)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m     67\u001B[0m     (fname \u001B[38;5;241m:=\u001B[39m get_allowed_file_name_or_none(module)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m     68\u001B[0m     (mtime \u001B[38;5;241m:=\u001B[39m os\u001B[38;5;241m.\u001B[39mstat(fname)\u001B[38;5;241m.\u001B[39mst_mtime) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_mtime_by_modname\u001B[38;5;241m.\u001B[39mget(\n\u001B[1;32m     69\u001B[0m         absolute_name, \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint):\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_hint \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 71\u001B[0m module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_builtins_import(name, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (fname \u001B[38;5;241m:=\u001B[39m fname \u001B[38;5;129;01mor\u001B[39;00m get_allowed_file_name_or_none(module)) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     73\u001B[0m     mtime \u001B[38;5;241m=\u001B[39m mtime \u001B[38;5;129;01mor\u001B[39;00m os\u001B[38;5;241m.\u001B[39mstat(fname)\u001B[38;5;241m.\u001B[39mst_mtime\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'dlt'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "param_environment = spark.conf.get(\"param_environment\", \"dev\")\n",
    "param_source_name = spark.conf.get(\"param_source_name\", \"\")\n",
    "schema = spark.conf.get(\"bronze_schema\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99670e33-f038-4666-959f-b0f32365dc3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f4eafd-34f8-4741-b5d0-2f2bc7252544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "errors_path = f\"/Volumes/bronze/raw/log/errors\"\n",
    "checkpoint_path = f\"/Volumes/bronze/raw/log/checkpoint\"\n",
    "source_path = f\"/Volumes/bronze/raw/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17f56a2-2bbf-4f2d-8d00-c84cb288aa08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tabele\n",
    "\n",
    "* **Live tables** są to materializowane widoki na lakehouse; zwracają wyniki przy każdym odświeżeniue \n",
    "* **Streaming live tables** są to table inkrementalne, oparte na streamie\n",
    "\n",
    "\n",
    "Podstawowa definicja DLT:\n",
    "\n",
    "**`@dlt.table`**<br/>\n",
    "**`def <function-name>():`**<br/>\n",
    "**`    return (<query>)`**</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e9d858-628e-4e17-985f-8af4addf0d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>bronze..</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>ISBN10</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>answered_questions</td>\n",
       "   <td>bigint</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>asin</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>availability</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>best_sellers_rank</td>\n",
       "   <td>array&lt;struct&lt;category:string,rank:bigint&gt;&gt;</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>brand</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>buybox_seller</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>categories</td>\n",
       "   <td>array&lt;string&gt;</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>currency</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>date_first_available</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>delivery</td>\n",
       "   <td>array&lt;string&gt;</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>department</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>description</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>discount</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>domain</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>features</td>\n",
       "   <td>array&lt;string&gt;</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>final_price</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>format</td>\n",
       "   <td>array&lt;struct&lt;name:string,price:string,url:string&gt;&gt;</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>image_url</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>images_count</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>initial_price</td>\n",
       "   <td>double</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>item_weight</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>manufacturer</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>model_number</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>plus_content</td>\n",
       "   <td>boolean</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>product_dimensions</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>rating</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>reviews_count</td>\n",
       "   <td>bigint</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>root_bs_rank</td>\n",
       "   <td>bigint</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>seller_id</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>seller_name</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>timestamp</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>title</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>upc</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>url</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>video</td>\n",
       "   <td>boolean</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>video_count</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>_rescued_data</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>_metadata</td>\n",
       "   <td>struct&lt;file_path:string,file_name:string,file_size:bigint,file_block_start:bigint,file_block_length:bigint,file_modification_time:timestamp&gt;</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>source_system</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>file_path</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>inserted_at</td>\n",
       "   <td>timestamp</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=1735726609950924#joblist/pipelines/create?initialSource=%2FShared%2FDLT%2FDLT%204.1.1%20-%20Bronze&redirectNotebookId=2274085522347997\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python\n",
    "import dlt\n",
    "from pyspark.sql.functions import lit, col, current_timestamp\n",
    "\n",
    "@dlt.table(\n",
    "    name=f\"bronze.{schema}.{param_source_name}\", \n",
    "    comment=\"Raw data ingested from cloud storage\"\n",
    ")\n",
    "def load_raw_files():\n",
    "    cloudfile = {\n",
    "        \"cloudFiles.format\": \"json\",\n",
    "        \"pathGlobFilter\": \"*.json\",\n",
    "        \"cloudFiles.inferColumnTypes\": \"true\",\n",
    "        \"cloudFiles.schemaLocation\": checkpoint_path\n",
    "    }\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "            .options(**cloudfile)\n",
    "            .option(\"checkpointLocation\", checkpoint_path)\n",
    "            .option(\"badRecordsPath\", errors_path)\n",
    "            .option(\"multiline\", True)\n",
    "            .load(source_path)\n",
    "            .selectExpr(\"*\", \"_metadata\")\n",
    "            .withColumn(\"source_system\", lit(param_source_name))\n",
    "            .withColumn(\"file_path\", col(\"_metadata.file_path\"))\n",
    "            .withColumn(\"inserted_at\", lit(current_timestamp()))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7010db18-83e2-4f10-8f4d-b764f308f970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Validating, Enriching, and Transforming Data\n",
    "\n",
    "DLT allows users to easily declare tables from results of any standard Spark transformations. DLT adds new functionality for data quality checks and provides a number of options to allow users to enrich the metadata for created tables.\n",
    "\n",
    "Let's break down the syntax of the query below.\n",
    "\n",
    "### Options for **`@dlt.table()`**\n",
    "\n",
    "There are <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-python-ref.html#create-table\" target=\"_blank\">a number of options</a> that can be specified during table creation. Here, we use two of these to annotate our dataset.\n",
    "\n",
    "##### **`comment`**\n",
    "\n",
    "Table comments are a standard for relational databases. They can be used to provide useful information to users throughout your organization. In this example, we write a short human-readable description of the table that describes how data is being ingested and enforced (which could also be gleaned from reviewing other table metadata).\n",
    "\n",
    "##### **`table_properties`**\n",
    "\n",
    "This field can be used to pass any number of key/value pairs for custom tagging of data. Here, we set the value **`silver`** for the key **`quality`**.\n",
    "\n",
    "Note that while this field allows for custom tags to be arbitrarily set, it is also used for configuring number of settings that control how a table will perform. While reviewing table details, you may also encounter a number of settings that are turned on by default any time a table is created.\n",
    "\n",
    "### Data Quality Constraints\n",
    "\n",
    "The Python version of DLT uses decorator functions to set <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-expectations.html#delta-live-tables-data-quality-constraints\" target=\"_blank\">data quality constraints</a>. We'll see a number of these throughout the course.\n",
    "\n",
    "DLT uses simple boolean statements to allow quality enforcement checks on data. In the statement below, we:\n",
    "* Declare a constraint named **`valid_date`**\n",
    "* Define the conditional check that the field **`order_timestamp`** must contain a value greater than January 1, 2021\n",
    "* Instruct DLT to fail the current transaction if any records violate the constraint by using the decorator **`@dlt.expect_or_fail()`**\n",
    "\n",
    "Each constraint can have multiple conditions, and multiple constraints can be set for a single table. In addition to failing the update, constraint violation can also automatically drop records or just record the number of violations while still processing these invalid records.\n",
    "\n",
    "### DLT Read Methods\n",
    "\n",
    "The Python **`dlt`** module provides the **`read()`** and **`read_stream()`** methods to easily configure references to other tables and views in your DLT Pipeline. This syntax allows you to reference these datasets by name without any database reference. You can also use **`spark.table(\"LIVE.<table_name.\")`**, where **`LIVE`** is a keyword substituted for the database being referenced in the DLT Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "360b3505-122c-491c-8493-fd2fef50dd34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Live Tables vs. Streaming Live Tables\n",
    "\n",
    "The two functions we've reviewed so far have both created streaming live tables. Below, we see a simple function that returns a live table (or materialized view) of some aggregated data.\n",
    "\n",
    "Spark has historically differentiated between batch queries and streaming queries. Live tables and streaming live tables have similar differences.\n",
    "\n",
    "Note that these table types inherit the syntax (as well as some of the limitations) of the PySpark and Structured Streaming APIs.\n",
    "\n",
    "Below are some of the differences between these types of tables.\n",
    "\n",
    "### Live Tables\n",
    "* Always \"correct\", meaning their contents will match their definition after any update.\n",
    "* Return same results as if table had just been defined for first time on all data.\n",
    "* Should not be modified by operations external to the DLT Pipeline (you'll either get undefined answers or your change will just be undone).\n",
    "\n",
    "### Streaming Live Tables\n",
    "* Only supports reading from \"append-only\" streaming sources.\n",
    "* Only reads each input batch once, no matter what (even if joined dimensions change, or if the query definition changes, etc).\n",
    "* Can perform operations on the table outside the managed DLT Pipeline (append data, perform GDPR, etc)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT_01_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}