{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c247678b-66ab-4879-b331-bb0532826530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Quickstart notebook\n",
    "The example code here shows how to get up and running with Mosaic using the Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf89a78-8b1f-4e27-8cc8-3fa382ff8210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec0b86d5-e58c-478a-824b-e1165fb267de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Enable Mosaic in the notebook\n",
    "To get started, you'll need to attach the python library to your cluster and execute the `enable_mosaic` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb6237c-050b-47c6-98ed-6942a50472e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n            DEPRECATION WARNING: \n                Please use a Databricks:\n                    - Photon-enabled Runtime for performance benefits\n                    - Runtime ML for spatial AI benefits\n                Mosaic will stop working on this cluster after v0.3.x.\n"
     ]
    }
   ],
   "source": [
    "from mosaic import enable_mosaic\n",
    "enable_mosaic(spark, dbutils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d774f893-1152-41de-adde-bdad37e755f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mosaic has extra configuration options. Check the docs for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10776858-768a-450f-9281-6a00e80d04c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function enable_mosaic in module mosaic.api.enable:\n\nenable_mosaic(spark: pyspark.sql.session.SparkSession, dbutils=None) -> None\n    Enable Mosaic functions.\n    \n    Use this function at the start of your workflow to ensure all the required dependencies are installed and\n    Mosaic is configured according to your needs.\n    \n    Parameters\n    ----------\n    spark : pyspark.sql.SparkSession\n            The active SparkSession.\n    dbutils : dbruntime.dbutils.DBUtils\n            The dbutils object used for `display` and `displayHTML` functions.\n            Optional, only applicable to Databricks users.\n    \n    Returns\n    -------\n    \n    Notes\n    -----\n    Users can control various aspects of Mosaic's operation with the following Spark confs:\n    \n    - `spark.databricks.labs.mosaic.jar.autoattach`: 'true' (default) or 'false'\n       Automatically attach the Mosaic JAR to the Databricks cluster? (Optional)\n    - `spark.databricks.labs.mosaic.jar.location`\n       Explicitly specify the path to the Mosaic JAR.\n       (Optional and not required at all in a standard Databricks environment).\n    - `spark.databricks.labs.mosaic.geometry.api`: 'JTS' (default) or 'ESRI'\n       Explicitly specify the underlying geometry library to use for spatial operations. (Optional)\n    - `spark.databricks.labs.mosaic.index.system`: 'H3' (default)\n       Explicitly specify the index system to use for optimized spatial joins. (Optional)\n\n"
     ]
    }
   ],
   "source": [
    "help(enable_mosaic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac3649d8-5561-4370-b5d3-a36bf5068fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Geometry constructors and the Mosaic internal geometry format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9e64d22-ebbc-4ea7-97aa-2dd6e240c9ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mosaic allows users to create new Point geometries from a pair of Spark DoubleType columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa85d1b2-8ec1-4879-87b4-cf55eb12820c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+\n| lat|  lon|          point_geom|\n+----+-----+--------------------+\n|35.0|-80.0|{1, 0, [[[-80.0, ...|\n|45.0|-80.0|{1, 0, [[[-80.0, ...|\n|45.0|-70.0|{1, 0, [[[-70.0, ...|\n|35.0|-70.0|{1, 0, [[[-70.0, ...|\n|35.0|-80.0|{1, 0, [[[-80.0, ...|\n+----+-----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from mosaic import st_point\n",
    "\n",
    "lons = [-80., -80., -70., -70., -80.]\n",
    "lats = [ 35.,  45.,  45.,  35.,  35.]\n",
    "\n",
    "bounds_df = (\n",
    "  spark\n",
    "  .createDataFrame({\"lon\": lon, \"lat\": lat} for lon, lat in zip(lons, lats))\n",
    "  .coalesce(1)\n",
    "  .withColumn(\"point_geom\", st_point(\"lon\", \"lat\"))\n",
    ")\n",
    "bounds_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0229f66-1fe7-4218-905f-6fe265fd2441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mosaic Point geometries can be aggregated into LineString and Polygon geometries using the respective constructors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c143e704-333d-4c15-800d-7dcefc924f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|       bounding_ring|\n+--------------------+\n|{3, 0, [[[-80.0, ...|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from mosaic import st_makeline\n",
    "\n",
    "bounds_df = (\n",
    "  bounds_df\n",
    "  .groupBy()\n",
    "  .agg(collect_list(\"point_geom\").alias(\"bounding_coords\"))\n",
    "  .select(st_makeline(\"bounding_coords\").alias(\"bounding_ring\"))\n",
    ")\n",
    "bounds_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0282fcbd-f9b1-4708-b015-dc9e0fac9072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|              bounds|\n+--------------------+\n|{5, 0, [[[-80.0, ...|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from mosaic import st_makepolygon\n",
    "\n",
    "bounds_df = bounds_df.select(st_makepolygon(\"bounding_ring\").alias(\"bounds\"))\n",
    "bounds_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "459a46e8-e7cb-4bb0-a167-e359889d15a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Geometry clipping without an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02d90b72-81b4-48a9-b117-b7976bca13aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mosaic implements set intersection functions: contains, intersects, overlaps etc. Here you can see `st_contains` being used to clip points by a polygon geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ec8d70-8d6b-44b9-ad2a-ef593e2011bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tripsTable = spark.table(\"delta.`/databricks-datasets/nyctaxi/tables/nyctaxi_yellow`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0e72a9-4e2e-45fe-ba39-d2933859154d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mosaic import st_contains\n",
    "trips = (\n",
    "  tripsTable\n",
    "  .limit(5_000_000)\n",
    "  .repartition(sc.defaultParallelism * 20)\n",
    "  .drop(\"vendorId\", \"rateCodeId\", \"store_and_fwd_flag\", \"payment_type\")\n",
    "  .withColumn(\"pickup_geom\", st_point(\"pickup_longitude\", \"pickup_latitude\"))\n",
    "  .withColumn(\"dropoff_geom\", st_point(\"dropoff_longitude\", \"dropoff_latitude\"))\n",
    "  .crossJoin(bounds_df)\n",
    "  .where(st_contains(\"bounds\", \"pickup_geom\"))\n",
    "  .where(st_contains(\"bounds\", \"dropoff_geom\"))\n",
    "  .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa22657-47c6-4de6-8e48-733a150b00a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+--------------------+--------------------+--------------------+\n|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|dropoff_longitude|dropoff_latitude|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|         pickup_geom|        dropoff_geom|              bounds|\n+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+--------------------+--------------------+--------------------+\n|      CMT|2009-10-31 22:18:30|2009-10-31 22:59:38|              2|          0.9|      -73.993177|       40.73217|        null|       -73.983302|       40.744515|       18.9|  0.5|    0.0|       0.0|         0.0|        19.4|{1, 0, [[[-73.993...|{1, 0, [[[-73.983...|{5, 0, [[[-80.0, ...|\n|      VTS|2009-04-18 01:12:00|2009-04-18 01:40:00|              4|         6.34|      -74.001658|      40.730848|        null|       -73.953983|       40.678067|       19.7|  0.5|   null|       0.0|         0.0|        20.2|{1, 0, [[[-74.001...|{1, 0, [[[-73.953...|{5, 0, [[[-80.0, ...|\n|      CMT|2009-01-23 18:44:13|2009-01-23 18:50:56|              3|          1.0|      -74.000804|      40.717094|        null|       -74.011118|       40.710257|        6.7|  0.0|   null|       1.0|         0.0|         7.7|{1, 0, [[[-74.000...|{1, 0, [[[-74.011...|{5, 0, [[[-80.0, ...|\n|      CMT|2009-10-24 16:15:06|2009-10-24 16:45:07|              2|          4.7|      -74.008309|      40.735454|        null|       -73.958334|       40.774746|       17.7|  0.0|    0.0|       0.0|         0.0|        17.7|{1, 0, [[[-74.008...|{1, 0, [[[-73.958...|{5, 0, [[[-80.0, ...|\n|      VTS|2009-04-30 22:32:00|2009-04-30 22:36:00|              1|         0.64|      -74.003943|      40.733402|        null|       -74.002673|       40.733575|        4.5|  0.5|   null|       0.0|         0.0|         5.0|{1, 0, [[[-74.003...|{1, 0, [[[-74.002...|{5, 0, [[[-80.0, ...|\n|      VTS|2009-12-27 13:33:00|2009-12-27 13:44:00|              5|         2.74|      -73.996308|      40.732043|        null|        -74.01739|       40.707303|        9.3|  0.0|    0.5|       0.0|         0.0|         9.8|{1, 0, [[[-73.996...|{1, 0, [[[-74.017...|{5, 0, [[[-80.0, ...|\n|      CMT|2009-06-19 21:08:26|2009-06-19 21:17:11|              1|          1.9|      -74.005336|      40.715127|        null|       -73.986924|        40.69753|        7.7|  0.0|   null|       0.0|         0.0|         7.7|{1, 0, [[[-74.005...|{1, 0, [[[-73.986...|{5, 0, [[[-80.0, ...|\n|      CMT|2009-04-29 21:33:20|2009-04-29 21:44:54|              1|          3.4|       -74.01159|      40.707776|        null|       -74.003782|       40.747518|       10.9|  0.0|   null|       0.0|         0.0|        10.9|{1, 0, [[[-74.011...|{1, 0, [[[-74.003...|{5, 0, [[[-80.0, ...|\n|      CMT|2009-09-15 08:28:09|2009-09-15 08:32:14|              1|          0.8|      -73.995942|      40.732548|        null|       -73.998899|       40.723132|        4.5|  0.0|   null|       0.0|         0.0|         4.5|{1, 0, [[[-73.995...|{1, 0, [[[-73.998...|{5, 0, [[[-80.0, ...|\n|      VTS|2009-07-15 14:45:00|2009-07-15 15:02:00|              1|         3.47|      -74.015295|      40.714598|        null|        -73.98937|       40.741883|       12.1|  0.0|   null|       3.0|         0.0|        15.1|{1, 0, [[[-74.015...|{1, 0, [[[-73.989...|{5, 0, [[[-80.0, ...|\n|      VTS|2009-05-13 12:12:00|2009-05-13 12:32:00|              4|         4.53|      -74.007792|      40.704722|        null|       -73.979948|       40.745698|       15.3|  0.0|   null|      3.06|         0.0|       18.36|{1, 0, [[[-74.007...|{1, 0, [[[-73.979...|{5, 0, [[[-80.0, ...|\n|      CMT|2009-10-28 22:29:06|2009-10-28 22:42:42|              2|          1.9|      -74.004492|      40.733942|        null|       -73.977958|       40.725299|        8.5|  0.5|    0.0|       1.8|         0.0|        10.8|{1, 0, [[[-74.004...|{1, 0, [[[-73.977...|{5, 0, [[[-80.0, ...|\n|      CMT|2009-12-29 17:37:13|2009-12-29 17:43:07|              1|          0.8|      -73.998091|      40.716608|        null|       -74.004562|       40.706446|        4.9|  1.0|    0.5|       0.0|         0.0|         6.4|{1, 0, [[[-73.998...|{1, 0, [[[-74.004...|{5, 0, [[[-80.0, ...|\n|      CMT|2009-05-19 12:45:04|2009-05-19 13:07:39|              1|          7.5|      -73.997364|      40.674552|        null|       -73.990399|       40.734621|       19.7|  0.0|   null|       0.0|         0.0|        19.7|{1, 0, [[[-73.997...|{1, 0, [[[-73.990...|{5, 0, [[[-80.0, ...|\n|      VTS|2010-02-22 21:17:00|2010-02-22 21:24:00|              1|         1.31|      -74.004452|      40.721307|           1|       -73.989715|        40.73015|        6.1|  0.5|    0.5|       0.0|         0.0|         7.1|{1, 0, [[[-74.004...|{1, 0, [[[-73.989...|{5, 0, [[[-80.0, ...|\n|      CMT|2009-10-30 19:36:47|2009-10-30 19:54:18|              2|          6.8|      -74.007404|      40.708664|        null|       -73.952606|       40.772021|       17.7|  1.0|    0.0|       0.0|         0.0|        18.7|{1, 0, [[[-74.007...|{1, 0, [[[-73.952...|{5, 0, [[[-80.0, ...|\n|      VTS|2009-12-11 19:58:00|2009-12-11 20:02:00|              1|          0.6|      -73.991923|       40.72588|        null|       -73.986362|       40.731203|        4.1|  1.0|    0.5|       0.0|         0.0|         5.6|{1, 0, [[[-73.991...|{1, 0, [[[-73.986...|{5, 0, [[[-80.0, ...|\n|      CMT|2009-05-13 15:45:28|2009-05-13 15:54:36|              1|          2.0|      -74.011124|      40.710601|        null|       -73.994657|       40.740228|        7.7|  0.0|   null|       1.3|         0.0|         9.0|{1, 0, [[[-74.011...|{1, 0, [[[-73.994...|{5, 0, [[[-80.0, ...|\n|      VTS|2009-11-19 22:42:00|2009-11-19 23:00:00|              5|          3.7|      -73.992218|      40.725543|        null|       -73.967325|       40.685427|       12.9|  0.5|    0.5|       0.0|         0.0|        13.9|{1, 0, [[[-73.992...|{1, 0, [[[-73.967...|{5, 0, [[[-80.0, ...|\n|      VTS|2009-07-08 08:00:00|2009-07-08 08:12:00|              1|         2.69|      -74.016425|      40.706817|        null|       -73.993423|        40.72757|        9.3|  0.0|   null|       2.0|         0.0|        11.3|{1, 0, [[[-74.016...|{1, 0, [[[-73.993...|{5, 0, [[[-80.0, ...|\n+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "trips.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9353ac7-aa07-4d75-8b16-0031803ea31e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read from GeoJson, compute some basic geometry attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db5daf84-fc48-45ce-81a7-5af76b7b6010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You've seen how Mosaic can create geometries from Spark native data types but it also provides functions to translate Well Known Text (WKT), Well Known Binary (WKB) and GeoJSON representations to Mosaic geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49505635-4e84-480e-9892-e3e816af2226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254747>:4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m st_geomfromgeojson\n",
       "\u001B[1;32m      3\u001B[0m geoJsonDF \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[0;32m----> 4\u001B[0m   spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m   \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/shared_uploads/stuart.lynn@databricks.com/NYC_Taxi_Zones.geojson\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m, st_geomfromgeojson(to_json(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m))))\n",
       "\u001B[1;32m      7\u001B[0m   \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperties.*\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      8\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape_area\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape_leng\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      9\u001B[0m )\n",
       "\u001B[1;32m     11\u001B[0m geoJsonDF\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n",
       "\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
       "\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/shared_uploads/stuart.lynn@databricks.com/NYC_Taxi_Zones.geojson."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254747>:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m st_geomfromgeojson\n\u001B[1;32m      3\u001B[0m geoJsonDF \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m----> 4\u001B[0m   spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m   \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/shared_uploads/stuart.lynn@databricks.com/NYC_Taxi_Zones.geojson\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m, st_geomfromgeojson(to_json(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m))))\n\u001B[1;32m      7\u001B[0m   \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperties.*\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape_area\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape_leng\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m )\n\u001B[1;32m     11\u001B[0m geoJsonDF\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/shared_uploads/stuart.lynn@databricks.com/NYC_Taxi_Zones.geojson.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/shared_uploads/stuart.lynn@databricks.com/NYC_Taxi_Zones.geojson.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mosaic import st_geomfromgeojson\n",
    "\n",
    "geoJsonDF = (\n",
    "  spark.read.format(\"json\")\n",
    "  .load(\"dbfs:/FileStore/shared_uploads/stuart.lynn@databricks.com/NYC_Taxi_Zones.geojson\")\n",
    "  .withColumn(\"geometry\", st_geomfromgeojson(to_json(col(\"geometry\"))))\n",
    "  .select(\"properties.*\", \"geometry\")\n",
    "  .drop(\"shape_area\", \"shape_leng\")\n",
    ")\n",
    "\n",
    "geoJsonDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "880c5fb4-c0b2-456b-9167-cb6327876dbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mosaic provides a number of functions for extracting the properties of geometries. Here are some that are relevant to Polygon geometries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939d35fd-d868-4e73-8338-3cb8142fffe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254749>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m st_area, st_length\n",
       "\u001B[1;32m      2\u001B[0m (\n",
       "\u001B[0;32m----> 3\u001B[0m   geoJsonDF\n",
       "\u001B[1;32m      4\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcalculatedArea\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mabs\u001B[39m(st_area(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n",
       "\u001B[1;32m      5\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcalculatedLength\u001B[39m\u001B[38;5;124m\"\u001B[39m, st_length(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcalculatedArea\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcalculatedLength\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      7\u001B[0m )\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'geoJsonDF' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254749>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m st_area, st_length\n\u001B[1;32m      2\u001B[0m (\n\u001B[0;32m----> 3\u001B[0m   geoJsonDF\n\u001B[1;32m      4\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcalculatedArea\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mabs\u001B[39m(st_area(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[1;32m      5\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcalculatedLength\u001B[39m\u001B[38;5;124m\"\u001B[39m, st_length(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcalculatedArea\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcalculatedLength\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m )\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mNameError\u001B[0m: name 'geoJsonDF' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'geoJsonDF' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mosaic import st_area, st_length\n",
    "(\n",
    "  geoJsonDF\n",
    "  .withColumn(\"calculatedArea\", abs(st_area(\"geometry\")))\n",
    "  .withColumn(\"calculatedLength\", st_length(\"geometry\"))\n",
    "  .select(\"geometry\", \"calculatedArea\", \"calculatedLength\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920599a2-6c52-40e4-8547-a99f757406a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254750>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mgeoJsonDF\u001B[49m\u001B[38;5;241m.\u001B[39mcount()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'geoJsonDF' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254750>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mgeoJsonDF\u001B[49m\u001B[38;5;241m.\u001B[39mcount()\n\n\u001B[0;31mNameError\u001B[0m: name 'geoJsonDF' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'geoJsonDF' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "geoJsonDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0d34f66-7498-4a86-9dd3-b2664ebc535c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Â Example point-in-poly with indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb687786-0494-468d-b2e4-dbe847ea887b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mosaic has built-in support for the popular spatial indexing library, H3. The user has access to functions for generating point indices and the sets of indices covering polygons, allowing point-in-polygon joins to be transformed into deterministic SQL joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef8a536-fda8-4a9a-a2e6-58ad0c39cc8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+--------------------+--------------------+--------------------+------------------+------------------+\n|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|dropoff_longitude|dropoff_latitude|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|         pickup_geom|        dropoff_geom|              bounds|         pickup_h3|        dropoff_h3|\n+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+--------------------+--------------------+--------------------+------------------+------------------+\n|      CMT|2009-10-31 22:18:30|2009-10-31 22:59:38|              2|          0.9|      -73.993177|       40.73217|        null|       -73.983302|       40.744515|       18.9|  0.5|    0.0|       0.0|         0.0|        19.4|{1, 0, [[[-73.993...|{1, 0, [[[-73.983...|{5, 0, [[[-80.0, ...|622236723432456191|622236723427082239|\n|      VTS|2009-04-18 01:12:00|2009-04-18 01:40:00|              4|         6.34|      -74.001658|      40.730848|        null|       -73.953983|       40.678067|       19.7|  0.5|   null|       0.0|         0.0|        20.2|{1, 0, [[[-74.001...|{1, 0, [[[-73.953...|{5, 0, [[[-80.0, ...|622236750719582207|622236723590955007|\n|      CMT|2009-01-23 18:44:13|2009-01-23 18:50:56|              3|          1.0|      -74.000804|      40.717094|        null|       -74.011118|       40.710257|        6.7|  0.0|   null|       1.0|         0.0|         7.7|{1, 0, [[[-74.000...|{1, 0, [[[-74.011...|{5, 0, [[[-80.0, ...|622236750709325823|622236750647558143|\n|      CMT|2009-10-24 16:15:06|2009-10-24 16:45:07|              2|          4.7|      -74.008309|      40.735454|        null|       -73.958334|       40.774746|       17.7|  0.0|    0.0|       0.0|         0.0|        17.7|{1, 0, [[[-74.008...|{1, 0, [[[-73.958...|{5, 0, [[[-80.0, ...|622236750532378623|622236722204803071|\n|      VTS|2009-04-30 22:32:00|2009-04-30 22:36:00|              1|         0.64|      -74.003943|      40.733402|        null|       -74.002673|       40.733575|        4.5|  0.5|   null|       0.0|         0.0|         5.0|{1, 0, [[[-74.003...|{1, 0, [[[-74.002...|{5, 0, [[[-80.0, ...|622236750719320063|622236750719254527|\n|      VTS|2009-12-27 13:33:00|2009-12-27 13:44:00|              5|         2.74|      -73.996308|      40.732043|        null|        -74.01739|       40.707303|        9.3|  0.0|    0.5|       0.0|         0.0|         9.8|{1, 0, [[[-73.996...|{1, 0, [[[-74.017...|{5, 0, [[[-80.0, ...|622236750715781119|622236750640218111|\n|      CMT|2009-06-19 21:08:26|2009-06-19 21:17:11|              1|          1.9|      -74.005336|      40.715127|        null|       -73.986924|        40.69753|        7.7|  0.0|   null|       0.0|         0.0|         7.7|{1, 0, [[[-74.005...|{1, 0, [[[-73.986...|{5, 0, [[[-80.0, ...|622236750708310015|622236750725152767|\n|      CMT|2009-04-29 21:33:20|2009-04-29 21:44:54|              1|          3.4|       -74.01159|      40.707776|        null|       -74.003782|       40.747518|       10.9|  0.0|   null|       0.0|         0.0|        10.9|{1, 0, [[[-74.011...|{1, 0, [[[-74.003...|{5, 0, [[[-80.0, ...|622236750647754751|622236750530904063|\n|      CMT|2009-09-15 08:28:09|2009-09-15 08:32:14|              1|          0.8|      -73.995942|      40.732548|        null|       -73.998899|       40.723132|        4.5|  0.0|   null|       0.0|         0.0|         4.5|{1, 0, [[[-73.995...|{1, 0, [[[-73.998...|{5, 0, [[[-80.0, ...|622236750715781119|622236750706999295|\n|      VTS|2009-07-15 14:45:00|2009-07-15 15:02:00|              1|         3.47|      -74.015295|      40.714598|        null|        -73.98937|       40.741883|       12.1|  0.0|   null|       3.0|         0.0|        15.1|{1, 0, [[[-74.015...|{1, 0, [[[-73.989...|{5, 0, [[[-80.0, ...|622236750652309503|622236723425935359|\n|      VTS|2009-05-13 12:12:00|2009-05-13 12:32:00|              4|         4.53|      -74.007792|      40.704722|        null|       -73.979948|       40.745698|       15.3|  0.0|   null|      3.06|         0.0|       18.36|{1, 0, [[[-74.007...|{1, 0, [[[-73.979...|{5, 0, [[[-80.0, ...|622236750650572799|622236723435175935|\n|      CMT|2009-10-28 22:29:06|2009-10-28 22:42:42|              2|          1.9|      -74.004492|      40.733942|        null|       -73.977958|       40.725299|        8.5|  0.5|    0.0|       1.8|         0.0|        10.8|{1, 0, [[[-74.004...|{1, 0, [[[-73.977...|{5, 0, [[[-80.0, ...|622236750719320063|622236723448184831|\n|      CMT|2009-12-29 17:37:13|2009-12-29 17:43:07|              1|          0.8|      -73.998091|      40.716608|        null|       -74.004562|       40.706446|        4.9|  1.0|    0.5|       0.0|         0.0|         6.4|{1, 0, [[[-73.998...|{1, 0, [[[-74.004...|{5, 0, [[[-80.0, ...|622236750708867071|622236750647328767|\n|      CMT|2009-05-19 12:45:04|2009-05-19 13:07:39|              1|          7.5|      -73.997364|      40.674552|        null|       -73.990399|       40.734621|       19.7|  0.0|   null|       0.0|         0.0|        19.7|{1, 0, [[[-73.997...|{1, 0, [[[-73.990...|{5, 0, [[[-80.0, ...|622236750657748991|622236723428556799|\n|      VTS|2010-02-22 21:17:00|2010-02-22 21:24:00|              1|         1.31|      -74.004452|      40.721307|           1|       -73.989715|        40.73015|        6.1|  0.5|    0.5|       0.0|         0.0|         7.1|{1, 0, [[[-74.004...|{1, 0, [[[-73.989...|{5, 0, [[[-80.0, ...|622236750706606079|622236750715518975|\n|      CMT|2009-10-30 19:36:47|2009-10-30 19:54:18|              2|          6.8|      -74.007404|      40.708664|        null|       -73.952606|       40.772021|       17.7|  1.0|    0.0|       0.0|         0.0|        18.7|{1, 0, [[[-74.007...|{1, 0, [[[-73.952...|{5, 0, [[[-80.0, ...|622236750646935551|622236722204540927|\n|      VTS|2009-12-11 19:58:00|2009-12-11 20:02:00|              1|          0.6|      -73.991923|       40.72588|        null|       -73.986362|       40.731203|        4.1|  1.0|    0.5|       0.0|         0.0|         5.6|{1, 0, [[[-73.991...|{1, 0, [[[-73.986...|{5, 0, [[[-80.0, ...|622236750714273791|622236723428917247|\n|      CMT|2009-05-13 15:45:28|2009-05-13 15:54:36|              1|          2.0|      -74.011124|      40.710601|        null|       -73.994657|       40.740228|        7.7|  0.0|   null|       1.3|         0.0|         9.0|{1, 0, [[[-74.011...|{1, 0, [[[-73.994...|{5, 0, [[[-80.0, ...|622236750647558143|622236723433406463|\n|      VTS|2009-11-19 22:42:00|2009-11-19 23:00:00|              5|          3.7|      -73.992218|      40.725543|        null|       -73.967325|       40.685427|       12.9|  0.5|    0.5|       0.0|         0.0|        13.9|{1, 0, [[[-73.992...|{1, 0, [[[-73.967...|{5, 0, [[[-80.0, ...|622236750714372095|622236723560480767|\n|      VTS|2009-07-08 08:00:00|2009-07-08 08:12:00|              1|         2.69|      -74.016425|      40.706817|        null|       -73.993423|        40.72757|        9.3|  0.0|   null|       2.0|         0.0|        11.3|{1, 0, [[[-74.016...|{1, 0, [[[-73.993...|{5, 0, [[[-80.0, ...|622236750640250879|622236750714044415|\n+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+-----------------+----------------+-----------+-----+-------+----------+------------+------------+--------------------+--------------------+--------------------+------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from mosaic import grid_longlatascellid\n",
    "\n",
    "trips_with_geom = (\n",
    "  trips\n",
    "  .withColumn(\"pickup_h3\", grid_longlatascellid(lon=\"pickup_longitude\", lat=\"pickup_latitude\", resolution=lit(10)))\n",
    "  .withColumn(\"dropoff_h3\", grid_longlatascellid(lon=\"dropoff_longitude\", lat=\"dropoff_latitude\", resolution=lit(10)))\n",
    ")\n",
    "\n",
    "trips_with_geom.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21627a45-71e0-4286-907b-94de6f04ec43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254754>:4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m grid_polyfill\n",
       "\u001B[1;32m      3\u001B[0m neighbourhoods \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[0;32m----> 4\u001B[0m   geoJsonDF\n",
       "\u001B[1;32m      5\u001B[0m   \u001B[38;5;241m.\u001B[39mrepartition(sc\u001B[38;5;241m.\u001B[39mdefaultParallelism)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m, explode(grid_polyfill(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m, lit(\u001B[38;5;241m10\u001B[39m)))\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh3\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m      7\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      8\u001B[0m )\n",
       "\u001B[1;32m     10\u001B[0m neighbourhoods\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'geoJsonDF' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254754>:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m grid_polyfill\n\u001B[1;32m      3\u001B[0m neighbourhoods \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m----> 4\u001B[0m   geoJsonDF\n\u001B[1;32m      5\u001B[0m   \u001B[38;5;241m.\u001B[39mrepartition(sc\u001B[38;5;241m.\u001B[39mdefaultParallelism)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m, explode(grid_polyfill(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m, lit(\u001B[38;5;241m10\u001B[39m)))\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh3\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      7\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m     10\u001B[0m neighbourhoods\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mNameError\u001B[0m: name 'geoJsonDF' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'geoJsonDF' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mosaic import grid_polyfill\n",
    "\n",
    "neighbourhoods = (\n",
    "  geoJsonDF\n",
    "  .repartition(sc.defaultParallelism)\n",
    "  .select(\"*\", explode(grid_polyfill(\"geometry\", lit(10))).alias(\"h3\"))\n",
    "  .drop(\"geometry\")\n",
    ")\n",
    "\n",
    "neighbourhoods.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3e68351-52d4-4a8b-8ce6-4898dedd0ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254755>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m joined_df \u001B[38;5;241m=\u001B[39m trips_with_geom\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mjoin(neighbourhoods\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m), on\u001B[38;5;241m=\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt.pickup_h3 = n.h3\u001B[39m\u001B[38;5;124m\"\u001B[39m), how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      2\u001B[0m joined_df\u001B[38;5;241m.\u001B[39mcount()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'neighbourhoods' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254755>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m joined_df \u001B[38;5;241m=\u001B[39m trips_with_geom\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mjoin(neighbourhoods\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m), on\u001B[38;5;241m=\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt.pickup_h3 = n.h3\u001B[39m\u001B[38;5;124m\"\u001B[39m), how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m joined_df\u001B[38;5;241m.\u001B[39mcount()\n\n\u001B[0;31mNameError\u001B[0m: name 'neighbourhoods' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'neighbourhoods' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_df = trips_with_geom.alias(\"t\").join(neighbourhoods.alias(\"n\"), on=expr(\"t.pickup_h3 = n.h3\"), how=\"inner\")\n",
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4fb8772-94b2-4fbe-aaf9-6e261609b009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mosaic spatial join optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7696c95-820a-434e-9535-48807e1fb27c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mosaic provides easy access to the optimized spatial join technique described in [this](https://databricks.com/blog/2021/10/11/efficient-point-in-polygon-joins-via-pyspark-and-bng-geospatial-indexing.html) blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1319c9ca-149c-4208-8512-f035ffff467b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254758>:4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m grid_tessellateexplode\n",
       "\u001B[1;32m      3\u001B[0m mosaic_neighbourhoods \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[0;32m----> 4\u001B[0m   geoJsonDF\n",
       "\u001B[1;32m      5\u001B[0m   \u001B[38;5;241m.\u001B[39mrepartition(sc\u001B[38;5;241m.\u001B[39mdefaultParallelism)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m, grid_tessellateexplode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m, lit(\u001B[38;5;241m10\u001B[39m)))\n",
       "\u001B[1;32m      7\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      8\u001B[0m )\n",
       "\u001B[1;32m     10\u001B[0m mosaic_neighbourhoods\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'geoJsonDF' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254758>:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m grid_tessellateexplode\n\u001B[1;32m      3\u001B[0m mosaic_neighbourhoods \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m----> 4\u001B[0m   geoJsonDF\n\u001B[1;32m      5\u001B[0m   \u001B[38;5;241m.\u001B[39mrepartition(sc\u001B[38;5;241m.\u001B[39mdefaultParallelism)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m, grid_tessellateexplode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m, lit(\u001B[38;5;241m10\u001B[39m)))\n\u001B[1;32m      7\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m     10\u001B[0m mosaic_neighbourhoods\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mNameError\u001B[0m: name 'geoJsonDF' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'geoJsonDF' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mosaic import grid_tessellateexplode\n",
    "\n",
    "mosaic_neighbourhoods = (\n",
    "  geoJsonDF\n",
    "  .repartition(sc.defaultParallelism)\n",
    "  .select(\"*\", grid_tessellateexplode(\"geometry\", lit(10)))\n",
    "  .drop(\"geometry\")\n",
    ")\n",
    "\n",
    "mosaic_neighbourhoods.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee5d5a0a-2f2f-4095-9ec8-ce3ba0af6eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mosaic also includes a convenience function for displaying dataframes with geometry columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162fc850-a289-416f-af6d-2dd73f30e93e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254760>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m displayMosaic\n",
       "\u001B[0;32m----> 2\u001B[0m displayMosaic(mosaic_neighbourhoods)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'mosaic_neighbourhoods' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254760>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m displayMosaic\n\u001B[0;32m----> 2\u001B[0m displayMosaic(mosaic_neighbourhoods)\n\n\u001B[0;31mNameError\u001B[0m: name 'mosaic_neighbourhoods' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'mosaic_neighbourhoods' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mosaic import displayMosaic\n",
    "displayMosaic(mosaic_neighbourhoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a304589f-36a7-4bef-8cd2-932ddd3acba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This also extends to plotting maps inside the notebook using the kepler.gl visualisation library using a notebook magic `%%mosaic_kepler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be96bd5-e14d-40bd-9fc7-00dea16cb096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254762>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m st_aswkt\n",
       "\u001B[1;32m      2\u001B[0m (\n",
       "\u001B[0;32m----> 3\u001B[0m   mosaic_neighbourhoods\n",
       "\u001B[1;32m      4\u001B[0m   \u001B[38;5;241m.\u001B[39mselect(st_aswkt(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex.wkb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwkt\u001B[39m\u001B[38;5;124m\"\u001B[39m), col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex.index_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh3\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m      5\u001B[0m )\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkepler_df\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'mosaic_neighbourhoods' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254762>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmosaic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m st_aswkt\n\u001B[1;32m      2\u001B[0m (\n\u001B[0;32m----> 3\u001B[0m   mosaic_neighbourhoods\n\u001B[1;32m      4\u001B[0m   \u001B[38;5;241m.\u001B[39mselect(st_aswkt(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex.wkb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwkt\u001B[39m\u001B[38;5;124m\"\u001B[39m), col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex.index_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh3\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      5\u001B[0m )\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkepler_df\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\n\u001B[0;31mNameError\u001B[0m: name 'mosaic_neighbourhoods' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'mosaic_neighbourhoods' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mosaic import st_aswkt\n",
    "(\n",
    "  mosaic_neighbourhoods\n",
    "  .select(st_aswkt(col(\"index.wkb\")).alias(\"wkt\"), col(\"index.index_id\").alias(\"h3\"))\n",
    ").createOrReplaceTempView(\"kepler_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe429bb-de6f-4118-8d7b-6484b6b2d678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/mosaic/utils/kepler_magic.py:107\u001B[0m, in \u001B[0;36mMosaicKepler.get_spark_df\u001B[0;34m(table_name)\u001B[0m\n",
       "\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 107\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmosaic_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable_name\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:473\u001B[0m, in \u001B[0;36mDataFrameReader.table\u001B[0;34m(self, tableName)\u001B[0m\n",
       "\u001B[1;32m    440\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m    441\u001B[0m \n",
       "\u001B[1;32m    442\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.4.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    471\u001B[0m \u001B[38;5;124;03m>>> _ = spark.sql(\"DROP TABLE tblA\")\u001B[39;00m\n",
       "\u001B[1;32m    472\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 473\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtableName\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `kepler_df` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n",
       "'UnresolvedRelation [kepler_df], [], false\n",
       "\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254763>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_cell_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmosaic_kepler\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mkepler_df\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mh3\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mh3\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2362\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n",
       "\u001B[1;32m   2360\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n",
       "\u001B[1;32m   2361\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n",
       "\u001B[0;32m-> 2362\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/mosaic/utils/kepler_magic.py:208\u001B[0m, in \u001B[0;36mMosaicKepler.mosaic_kepler\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(inputs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n",
       "\u001B[1;32m    207\u001B[0m     limit_ctn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(inputs[\u001B[38;5;241m3\u001B[39m])\n",
       "\u001B[0;32m--> 208\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_spark_df\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable_name\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    209\u001B[0m feature_col_dt \u001B[38;5;241m=\u001B[39m [dt \u001B[38;5;28;01mfor\u001B[39;00m dt \u001B[38;5;129;01min\u001B[39;00m data\u001B[38;5;241m.\u001B[39mdtypes \u001B[38;5;28;01mif\u001B[39;00m dt[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m feature_name][\u001B[38;5;241m0\u001B[39m]\n",
       "\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m feature_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh3\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/mosaic/utils/kepler_magic.py:109\u001B[0m, in \u001B[0;36mMosaicKepler.get_spark_df\u001B[0;34m(table_name)\u001B[0m\n",
       "\u001B[1;32m    107\u001B[0m         data \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mmosaic_spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(table_name)\n",
       "\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
       "\u001B[0;32m--> 109\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTable name reference invalid.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
       "\n",
       "\u001B[0;31mException\u001B[0m: Table name reference invalid."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/mosaic/utils/kepler_magic.py:107\u001B[0m, in \u001B[0;36mMosaicKepler.get_spark_df\u001B[0;34m(table_name)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 107\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmosaic_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:473\u001B[0m, in \u001B[0;36mDataFrameReader.table\u001B[0;34m(self, tableName)\u001B[0m\n\u001B[1;32m    440\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m    441\u001B[0m \n\u001B[1;32m    442\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.4.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    471\u001B[0m \u001B[38;5;124;03m>>> _ = spark.sql(\"DROP TABLE tblA\")\u001B[39;00m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 473\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtableName\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\n\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `kepler_df` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [kepler_df], [], false\n\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254763>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_cell_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmosaic_kepler\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mkepler_df\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mh3\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mh3\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2362\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2360\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2361\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2362\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/mosaic/utils/kepler_magic.py:208\u001B[0m, in \u001B[0;36mMosaicKepler.mosaic_kepler\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(inputs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\u001B[1;32m    207\u001B[0m     limit_ctn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(inputs[\u001B[38;5;241m3\u001B[39m])\n\u001B[0;32m--> 208\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_spark_df\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m feature_col_dt \u001B[38;5;241m=\u001B[39m [dt \u001B[38;5;28;01mfor\u001B[39;00m dt \u001B[38;5;129;01min\u001B[39;00m data\u001B[38;5;241m.\u001B[39mdtypes \u001B[38;5;28;01mif\u001B[39;00m dt[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m feature_name][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m feature_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh3\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/mosaic/utils/kepler_magic.py:109\u001B[0m, in \u001B[0;36mMosaicKepler.get_spark_df\u001B[0;34m(table_name)\u001B[0m\n\u001B[1;32m    107\u001B[0m         data \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mmosaic_spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(table_name)\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m--> 109\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTable name reference invalid.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n\n\u001B[0;31mException\u001B[0m: Table name reference invalid.",
       "errorSummary": "<span class='ansi-red-fg'>Exception</span>: Table name reference invalid.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%mosaic_kepler\n",
    "\"kepler_df\" \"h3\" \"h3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4323fa2c-8495-43c6-afb2-9e239e5fb6fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![mosaic kepler map example](../images/kepler-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11933dab-085c-4503-8ba1-06365caa335c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now the two datasets can be joined first on H3 index, with any false positives removed through a contains filter on a much simpler geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98514998-1fe7-4a0c-b4f9-eebf38fb9d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254766>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m mosaic_joined_df \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m      2\u001B[0m   trips_with_geom\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 3\u001B[0m   \u001B[38;5;241m.\u001B[39mjoin(mosaic_neighbourhoods\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m), on\u001B[38;5;241m=\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt.pickup_h3 = n.index.index_id\u001B[39m\u001B[38;5;124m\"\u001B[39m), how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      4\u001B[0m   \u001B[38;5;241m.\u001B[39mwhere(\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m~\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex.is_core\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m|\u001B[39m \n",
       "\u001B[1;32m      6\u001B[0m     st_contains(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex.wkb\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpickup_geom\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      7\u001B[0m   )\n",
       "\u001B[1;32m      8\u001B[0m )\n",
       "\u001B[1;32m     10\u001B[0m mosaic_joined_df\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'mosaic_neighbourhoods' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254766>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m mosaic_joined_df \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m   trips_with_geom\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m   \u001B[38;5;241m.\u001B[39mjoin(mosaic_neighbourhoods\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m), on\u001B[38;5;241m=\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt.pickup_h3 = n.index.index_id\u001B[39m\u001B[38;5;124m\"\u001B[39m), how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m   \u001B[38;5;241m.\u001B[39mwhere(\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m~\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex.is_core\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m|\u001B[39m \n\u001B[1;32m      6\u001B[0m     st_contains(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex.wkb\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpickup_geom\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m   )\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m     10\u001B[0m mosaic_joined_df\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mNameError\u001B[0m: name 'mosaic_neighbourhoods' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'mosaic_neighbourhoods' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mosaic_joined_df = (\n",
    "  trips_with_geom.alias(\"t\")\n",
    "  .join(mosaic_neighbourhoods.alias(\"n\"), on=expr(\"t.pickup_h3 = n.index.index_id\"), how=\"inner\")\n",
    "  .where(\n",
    "    ~col(\"index.is_core\") | \n",
    "    st_contains(\"index.wkb\", \"pickup_geom\")\n",
    "  )\n",
    ")\n",
    "\n",
    "mosaic_joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ad416fa-3a36-4946-98f9-b8d306d31dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MosaicFrame abstraction for simple indexing and joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aba08cb2-fbbb-4de7-acdc-5eac0c12b91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "By wrapping our Spark DataFrames with `MosaicFrame`, we can simplify the join process. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54194240-dc88-42dc-a7e0-8b4a97a496f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mosaic import MosaicFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94730cc1-afc1-4227-9c91-709a18bb6eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:149: UserWarning: DataFrame constructor is internal. Do not directly use it.\n  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254770>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m trips_mdf \u001B[38;5;241m=\u001B[39m MosaicFrame(trips, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpickup_geom\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 2\u001B[0m neighbourhoods_mdf \u001B[38;5;241m=\u001B[39m MosaicFrame(geoJsonDF, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'geoJsonDF' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254770>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m trips_mdf \u001B[38;5;241m=\u001B[39m MosaicFrame(trips, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpickup_geom\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m neighbourhoods_mdf \u001B[38;5;241m=\u001B[39m MosaicFrame(geoJsonDF, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeometry\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\n\u001B[0;31mNameError\u001B[0m: name 'geoJsonDF' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'geoJsonDF' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trips_mdf = MosaicFrame(trips, \"pickup_geom\")\n",
    "neighbourhoods_mdf = MosaicFrame(geoJsonDF, \"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33782bb-3d4f-44e8-a33b-faeb11d0de46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2717610116254771>:6\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m (\n",
       "\u001B[1;32m      2\u001B[0m   trips_mdf\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;241m.\u001B[39mset_index_resolution(\u001B[38;5;241m10\u001B[39m)\n",
       "\u001B[1;32m      4\u001B[0m   \u001B[38;5;241m.\u001B[39mapply_index()\n",
       "\u001B[1;32m      5\u001B[0m   \u001B[38;5;241m.\u001B[39mjoin(\n",
       "\u001B[0;32m----> 6\u001B[0m     \u001B[43mneighbourhoods_mdf\u001B[49m\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mset_index_resolution(\u001B[38;5;241m10\u001B[39m)\n",
       "\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39mapply_index()\n",
       "\u001B[1;32m      9\u001B[0m   )\n",
       "\u001B[1;32m     10\u001B[0m )\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'neighbourhoods_mdf' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2717610116254771>:6\u001B[0m\n\u001B[1;32m      1\u001B[0m (\n\u001B[1;32m      2\u001B[0m   trips_mdf\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;241m.\u001B[39mset_index_resolution(\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m      4\u001B[0m   \u001B[38;5;241m.\u001B[39mapply_index()\n\u001B[1;32m      5\u001B[0m   \u001B[38;5;241m.\u001B[39mjoin(\n\u001B[0;32m----> 6\u001B[0m     \u001B[43mneighbourhoods_mdf\u001B[49m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mset_index_resolution(\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39mapply_index()\n\u001B[1;32m      9\u001B[0m   )\n\u001B[1;32m     10\u001B[0m )\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mNameError\u001B[0m: name 'neighbourhoods_mdf' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'neighbourhoods_mdf' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(\n",
    "  trips_mdf\n",
    "  .set_index_resolution(10)\n",
    "  .apply_index()\n",
    "  .join(\n",
    "    neighbourhoods_mdf\n",
    "    .set_index_resolution(10)\n",
    "    .apply_index()\n",
    "  )\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Mosaic",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}